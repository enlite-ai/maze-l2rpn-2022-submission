# @package runner

# Where to save the best model (output directory handled by hydra)
state_dict_dump_file: "state_dict.pt"


# Config arguments used for initializing ray (ray.init(**ray_config))
ray_config:
  # (str): The address of the Ray cluster to connect to. If this address is not provided, then this command will start
  #  Redis, a raylet, a plasma store, a plasma manager, and some workers. It will also kill these processes when Python
  #  exits. If the driver is running on a node in a Ray cluster, using `auto` as the value tells the driver to detect
  #  the the cluster, removing the need to specify a specific node address.
  address: ~

  # (int): Number of CPUs the user wishes to assign to each raylet. By default, this is set based on virtual cores.
  num_cpus: ~

  # (int): Number of GPUs the user wishes to assign to each raylet. By default, this is set based on detected GPUs.
  num_gpus: ~

  # A dictionary mapping the names of custom resources to the quantities for them available.
  resources: ~

  # The amount of memory (in bytes) to start the object store with. By default, this is automatically set based on
  #  available system memory.
  object_store_memory: ~

  # (bool): If true, the code will be executed serially. This is useful for debugging.
  local_mode: false

  # If true, Ray suppresses errors from calling ray.init() a second time. Ray won't be restarted.
  ignore_reinit_error: false

  # Boolean flag indicating whether or not to start the Ray dashboard, which displays the status of the Ray cluster. If
  #  this argument is None, then the UI will be started if the relevant dependencies are present.
  include_dashboard: ~

  # The host to bind the dashboard server to. Can either be localhost (127.0.0.1) or 0.0.0.0 (available from all
  #  interfaces). By default, this is set to localhost to prevent access from external machines.
  dashboard_host: "127.0.0.1"

  # The port to bind the dashboard server to. Defaults to 8265.
  dashboard_port: 8265

  # (ray.job_config.JobConfig): The job configuration.
  job_config: ~

  # true (default) if configuration of logging is allowed here. Otherwise, the user may want to configure it separately.
  configure_logging: true

  # Logging level, defaults to logging.INFO. Ignored unless "configure_logging" is true.
  logging_level: 20

  # Logging format, defaults to string containing a timestamp, filename, line number, and message. See the source file
  #  ray_constants.py for details. Ignored unless "configure_logging" is true.
  logging_format: ("%(asctime)s\t%(levelname)s %(filename)s:%(lineno)s -- %(message)s")

  # (bool): If true, the output from all of the worker processes on all nodes will be directed to the driver.
  log_to_driver: true


# Config arguments for starting a tune run. (tune.run(cfg.algorithm.algorithm, **tune_config, config=rllib_config))
tune_config:

  # (str): Name of experiment.
  name: ~

  # (str): Metric to optimize. This metric should be reported with `tune.report()`. If set, will be passed to the search
  #  algorithm and scheduler.
  metric: ~

  # (str): Must be one of [min, max]. Determines whether objective is minimizing or maximizing the metric attribute. If
  #  set, will be passed to the search algorithm and scheduler.
  mode: ~

  # (dict | callable | :class:`Stopper`): Stopping criteria. If dict, the keys may be any field in the return result of
  #  'train()', whichever is reached first. If function, it must take (trial_id, result) as arguments and return a
  #  boolean (true if trial should be stopped, false otherwise). This can also be a subclass of ``ray.tune.Stopper``,
  #  which allows users to implement custom experiment-wide stopping (i.e., stopping an entire Tune run based on some
  #  time constraint).
  stop: ~

  # (int|float|datetime.timedelta): Global time budget in seconds after which all trials are stopped. Can also be a
  #  ``datetime.timedelta`` object.
  time_budget_s: ~

  # Config will be loaded from the algorithm package
  #config: ~

  # (dict): Machine resources to allocate per trial, e.g. ``{"cpu": 64, "gpu": 8}``. Note that GPUs will not be assigned
  #  unless you specify them here. Defaults to 1 CPU and 0 GPUs in ``Trainable.default_resource_request()``.
  resources_per_trial: ~

  # (int): Number of times to sample from the hyperparameter space. Defaults to 1. If `grid_search` is provided as an
  #  argument, the grid will be repeated `num_samples` of times. If this is -1, (virtually) infinite samples are
  #  generated until a stopping condition is met.
  num_samples: 1

  # (str): Local dir to save training results to. Defaults to ``./``. Since the Maze hydra config already creates a new
  #  directory.
  local_dir: './'

  # (Searcher): Search algorithm for optimization.
  search_alg: ~

  # (TrialScheduler): Scheduler for executing the experiment. Choose among FIFO (default), MedianStopping,
  #  AsyncHyperBand, HyperBand and PopulationBasedTraining. Refer to ray.tune.schedulers for more options.
  scheduler: ~

  # (int): Number of checkpoints to keep. A value of `None` keeps all checkpoints. Defaults to `None`. If set, need
  #  to provide `checkpoint_score_attr`.
  keep_checkpoints_num: ~

  # (str): Specifies by which attribute to rank the best checkpoint. Default is increasing order. If attribute starts
  #  with `min-` it will rank attribute in decreasing order, i.e. `min-validation_loss`.
  checkpoint_score_attr: ~

  # (int): How many training iterations between checkpoints. A value of 0 (default) disables checkpointing. This has no
  #  effect when using the Functional Training API.
  checkpoint_freq: 0

  # (bool): Whether to checkpoint at the end of the experiment regardless of the checkpoint_freq. Default is false. This
  #  has no effect when using the Functional Training API.
  checkpoint_at_end: false

  # (int): 0, 1, or 2. Verbosity mode. 0 = silent, 1 = only status updates, 2 = status and trial results.
  verbose: 2

  # (ProgressReporter): Progress reporter for reporting intermediate experiment progress. Defaults to CLIReporter if
  #  running in command-line, or JupyterNotebookReporter if running in a Jupyter notebook.
  progress_reporter: ~

  # (list): List of logger creators to be used with each Trial. If None, defaults to ray.tune.logger.DEFAULT_LOGGERS.
  #  See `ray/tune/logger.py`.
  loggers: ~

  # (bool|str|Sequence): Log stdout and stderr to files in Tune's trial directories. If this is `false` (default), no
  #  files are written. If `true`, outputs are written to `trialdir/stdout` and `trialdir/stderr`, respectively. If this
  #  is a single string, this is interpreted as a file relative to the trialdir, to which both streams are written. If
  #  this is a Sequence (e.g. a Tuple), it has to have length 2 and the elements indicate the files to which stdout
  #  and stderr are written, respectively.
  log_to_file: false

  # (Callable[[Trial], str]): Optional function for generating the trial string representation.
  trial_name_creator: ~

  # (Callable[[Trial], str]): Function for generating the trial dirname. This function should take in a Trial object and
  #  return a string representing the name of the directory. The return value cannot be a path.
  trial_dirname_creator: ~

  # (SyncConfig): Configuration object for syncing. See tune.SyncConfig.
  sync_config: ~

  # (list): List of formats that exported at the end of the experiment. Default is None.
  export_formats: ~

  # (int): Try to recover a trial at least this many times. Ray will recover from the latest checkpoint if present.
  #  Setting to -1 will lead to infinite recovery retries. Setting to 0 will disable retries. Defaults to 0.
  max_failures: 0

  # (bool | str): Whether to fail upon the first error. If fail_fast='raise' provided, Tune will automatically raise the
  #  exception received by the Trainable. fail_fast='raise' can easily leak resources and should be used with caution
  #  (it is best used with `ray.init(local_mode=true)`).
  fail_fast: false

  # (str): Path to checkpoint. Only makes sense to set if running 1 trial. Defaults to None.
  restore: ~

  # (int): Port number for launching TuneServer.
  server_port: ~

  # (str|bool): One of "LOCAL", "REMOTE", "PROMPT", "ERRORED_ONLY", or bool. LOCAL/true restores the checkpoint from the
  #  local_checkpoint_dir, determined by `name` and `local_dir`. REMOTE restores the checkpoint from
  #  remote_checkpoint_dir. PROMPT provides CLI feedback. false forces a new experiment. ERRORED_ONLY resets and reruns
  #  ERRORED trials upon resume - previous trial artifacts will be left untouched.  If resume is set but checkpoint does
  #  not exist, ValueError will be thrown.
  resume: false

  # (bool): Whether to queue trials when the cluster does not currently have enough resources to launch one. This should
  #  be set to true when running on an autoscaling cluster to enable automatic scale-up.
  queue_trials: false

  # (bool): Whether to reuse actors between different trials when possible. This can drastically speed up experiments
  #  that start and stop actors often (e.g., PBT in time-multiplexing mode). This requires trials to have the same
  #  resource requirements.
  reuse_actors: false

  # (TrialExecutor): Manage the execution of trials.
  trial_executor: ~

  # (bool): Raise TuneError if there exists failed trial (of ERROR state) when the experiments complete.
  raise_on_failed_trial: true

  # (list): List of callbacks that will be called at different times in the training loop. Must be instances of the
  #  ``ray.tune.trial_runner.Callback`` class.
  callbacks: ~